{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def goto(linenum):\n",
    "    global line\n",
    "    line = linenum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIKIPYDIA: My name is WIKIPYDIA. What do you want to know about?\n",
      "MS Dhoni\n",
      "WIKIPYDIA: I will answer your queries about MS Dhoni. If you want to exit, type Bye!\n",
      "how many runs did he scored in his odi debut\n",
      "WIKIPYDIA: dhoni did not have a great start to his odi career, getting run out for a duck on debut.\n",
      "against which team did he made his debut\n",
      "WIKIPYDIA: dhoni did not have a great start to his odi career, getting run out for a duck on debut.\n",
      "when he started captaincy\n",
      "WIKIPYDIA: as the batsman started to walk back, captain dravid declared the innings when the confusion started as the umpires were not certain if the fielder stepped on the ropes and dhoni stayed for the umpire's verdict.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import wikipedia\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "try:\n",
    "    query = input('WIKIPYDIA: My name is WIKIPYDIA. What do you want to know about?\\n')\n",
    "    raw = wikipedia.page(query).content\n",
    "\n",
    "    raw=raw.lower()# converts to lowercase\n",
    "    sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "    word_tokens = nltk.word_tokenize(raw)# converts to list of words\n",
    "\n",
    "    lemmer = nltk.stem.WordNetLemmatizer()\n",
    "    def LemTokens(tokens):\n",
    "        return [lemmer.lemmatize(token) for token in tokens]\n",
    "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "    def LemNormalize(text):\n",
    "        return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "\n",
    "    GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "    GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "\n",
    "\n",
    "    # Checking for greetings\n",
    "    def greeting(sentence):\n",
    "    #If user's input is a greeting, return a greeting response\n",
    "        for word in sentence.split():\n",
    "            if word.lower() in GREETING_INPUTS:\n",
    "                return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "        # Generating response\n",
    "    def response(user_response):\n",
    "        robo_response=''\n",
    "        sent_tokens.append(user_response)\n",
    "        TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "        tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "        vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "        idx=vals.argsort()[0][-2]\n",
    "        flat = vals.flatten()\n",
    "        flat.sort()\n",
    "        req_tfidf = flat[-2]\n",
    "        if(req_tfidf==0):\n",
    "            robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "            return robo_response\n",
    "        else:\n",
    "            robo_response = robo_response+sent_tokens[idx]\n",
    "            return robo_response\n",
    "\n",
    "\n",
    "    flag=True\n",
    "    print(\"WIKIPYDIA: I will answer your queries about \"+query+\". If you want to exit, type Bye!\")\n",
    "\n",
    "    while(flag==True):\n",
    "        user_response = input()\n",
    "        user_response=user_response.lower()\n",
    "        if(user_response!='bye'):\n",
    "            if(user_response=='thanks' or user_response=='thank you' ):\n",
    "                flag=False\n",
    "                print(\"WIKIPYDIA: You are welcome..\")\n",
    "            else:\n",
    "                if(greeting(user_response)!=None):\n",
    "                    print(\"WIKIPYDIA: \"+greeting(user_response))\n",
    "                else:\n",
    "                    print(\"WIKIPYDIA: \",end=\"\")\n",
    "                    print(response(user_response))\n",
    "                    sent_tokens.remove(user_response)\n",
    "        else:\n",
    "            flag=False\n",
    "            print(\"WIKIPYDIA: Bye! take care..\")\n",
    "except wikipedia.DisambiguationError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
