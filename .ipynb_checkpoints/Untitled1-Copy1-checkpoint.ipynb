{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T13:34:50.702846Z",
     "start_time": "2019-07-28T13:34:44.753042Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import wikipedia\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T13:43:48.259613Z",
     "start_time": "2019-07-28T13:34:50.705831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jon Snow: What do you want to know about?\n",
      "You: Tyrion Lannister\n",
      "Jon Snow: I will answer your queries about Tyrion Lannister\n"
     ]
    }
   ],
   "source": [
    "botname = 'Jon Snow'\n",
    "query = input(f'{botname}: What do you want to know about?\\nYou: ')\n",
    "search_results = wikipedia.search(query.lower())\n",
    "if len(search_results) > 0:\n",
    "    print(f'{botname}: I will answer your queries about {search_results[0]}')\n",
    "    fetched_content = wikipedia.page(query).content\n",
    "else:\n",
    "    print('Not Found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T14:49:13.890124Z",
     "start_time": "2019-07-28T14:49:13.883144Z"
    }
   },
   "outputs": [],
   "source": [
    "content = ''\n",
    "for sent in fetched_content.split('\\n'):\n",
    "    if len(sent) > 0 and not sent.startswith('='):\n",
    "        content += sent + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T14:49:58.356812Z",
     "start_time": "2019-07-28T14:49:58.327889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokenize(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T13:43:48.480134Z",
     "start_time": "2019-07-28T13:43:48.269585Z"
    }
   },
   "outputs": [],
   "source": [
    "def separate_punc(text):\n",
    "    return [x.lower() for x in word_tokenize(fetched_content) if re.match(r'\\w+', x)]\n",
    "\n",
    "tokens = separate_punc(fetched_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T13:43:48.497093Z",
     "start_time": "2019-07-28T13:43:48.485122Z"
    }
   },
   "outputs": [],
   "source": [
    "train_len = 15+1\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T13:43:48.782630Z",
     "start_time": "2019-07-28T13:43:48.501077Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T13:43:48.795595Z",
     "start_time": "2019-07-28T13:43:48.786616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_counts)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T13:43:48.944762Z",
     "start_time": "2019-07-28T13:43:48.802572Z"
    }
   },
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)\n",
    "\n",
    "X = sequences[:, :-1]\n",
    "y = sequences[:, -1]\n",
    "\n",
    "y = to_categorical(y, num_classes=vocab_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T13:43:49.097347Z",
     "start_time": "2019-07-28T13:43:48.948665Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T13:43:49.230930Z",
     "start_time": "2019-07-28T13:43:49.102333Z"
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1)\n",
    "filepath = \"model.h5\"\n",
    "ckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T13:43:49.398720Z",
     "start_time": "2019-07-28T13:43:49.233894Z"
    }
   },
   "outputs": [],
   "source": [
    "def nn(vocab_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, seq_len, input_length=seq_len))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dense(2408, activation='relu'))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T13:43:50.696962Z",
     "start_time": "2019-07-28T13:43:49.401712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\Ritvik\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 15, 15)            28635     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 15, 32)            6144      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               295936    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2408)              618856    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1909)              4598781   \n",
      "=================================================================\n",
      "Total params: 5,548,352\n",
      "Trainable params: 5,548,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = nn(vocab_size+1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T13:58:03.653909Z",
     "start_time": "2019-07-28T13:43:50.700950Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\Ritvik\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "7233/7233 [==============================] - 38s 5ms/step - loss: 6.5904 - acc: 0.0423\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.59045, saving model to model.h5\n",
      "Epoch 2/500\n",
      "7233/7233 [==============================] - 33s 5ms/step - loss: 6.2055 - acc: 0.0563\n",
      "\n",
      "Epoch 00002: loss improved from 6.59045 to 6.20553, saving model to model.h5\n",
      "Epoch 3/500\n",
      "7233/7233 [==============================] - 34s 5ms/step - loss: 6.1433 - acc: 0.0563\n",
      "\n",
      "Epoch 00003: loss improved from 6.20553 to 6.14335, saving model to model.h5\n",
      "Epoch 4/500\n",
      "7233/7233 [==============================] - 33s 5ms/step - loss: 5.9993 - acc: 0.0585\n",
      "\n",
      "Epoch 00004: loss improved from 6.14335 to 5.99927, saving model to model.h5\n",
      "Epoch 5/500\n",
      "7233/7233 [==============================] - 33s 4ms/step - loss: 5.7331 - acc: 0.0709\n",
      "\n",
      "Epoch 00005: loss improved from 5.99927 to 5.73306, saving model to model.h5\n",
      "Epoch 6/500\n",
      "7233/7233 [==============================] - 31s 4ms/step - loss: 5.4369 - acc: 0.0787\n",
      "\n",
      "Epoch 00006: loss improved from 5.73306 to 5.43688, saving model to model.h5\n",
      "Epoch 7/500\n",
      "7233/7233 [==============================] - 25s 4ms/step - loss: 5.1776 - acc: 0.0848\n",
      "\n",
      "Epoch 00007: loss improved from 5.43688 to 5.17762, saving model to model.h5\n",
      "Epoch 8/500\n",
      "7233/7233 [==============================] - 28s 4ms/step - loss: 4.9382 - acc: 0.0955\n",
      "\n",
      "Epoch 00008: loss improved from 5.17762 to 4.93818, saving model to model.h5\n",
      "Epoch 9/500\n",
      "7233/7233 [==============================] - 35s 5ms/step - loss: 4.7262 - acc: 0.1073\n",
      "\n",
      "Epoch 00009: loss improved from 4.93818 to 4.72616, saving model to model.h5\n",
      "Epoch 10/500\n",
      "7233/7233 [==============================] - 29s 4ms/step - loss: 4.4970 - acc: 0.1096\n",
      "\n",
      "Epoch 00010: loss improved from 4.72616 to 4.49701, saving model to model.h5\n",
      "Epoch 11/500\n",
      "7233/7233 [==============================] - 28s 4ms/step - loss: 4.2296 - acc: 0.1329\n",
      "\n",
      "Epoch 00011: loss improved from 4.49701 to 4.22956, saving model to model.h5\n",
      "Epoch 12/500\n",
      "7233/7233 [==============================] - 26s 4ms/step - loss: 3.9625 - acc: 0.1493\n",
      "\n",
      "Epoch 00012: loss improved from 4.22956 to 3.96253, saving model to model.h5\n",
      "Epoch 13/500\n",
      "7233/7233 [==============================] - 27s 4ms/step - loss: 3.6473 - acc: 0.1741\n",
      "\n",
      "Epoch 00013: loss improved from 3.96253 to 3.64728, saving model to model.h5\n",
      "Epoch 14/500\n",
      "7233/7233 [==============================] - 27s 4ms/step - loss: 3.3366 - acc: 0.2108\n",
      "\n",
      "Epoch 00014: loss improved from 3.64728 to 3.33665, saving model to model.h5\n",
      "Epoch 15/500\n",
      "7233/7233 [==============================] - 28s 4ms/step - loss: 3.0066 - acc: 0.2632\n",
      "\n",
      "Epoch 00015: loss improved from 3.33665 to 3.00657, saving model to model.h5\n",
      "Epoch 16/500\n",
      "7233/7233 [==============================] - 37s 5ms/step - loss: 2.6773 - acc: 0.3232\n",
      "\n",
      "Epoch 00016: loss improved from 3.00657 to 2.67727, saving model to model.h5\n",
      "Epoch 17/500\n",
      "7233/7233 [==============================] - 30s 4ms/step - loss: 2.3652 - acc: 0.3917\n",
      "\n",
      "Epoch 00017: loss improved from 2.67727 to 2.36524, saving model to model.h5\n",
      "Epoch 18/500\n",
      "7233/7233 [==============================] - 29s 4ms/step - loss: 2.0583 - acc: 0.4517\n",
      "\n",
      "Epoch 00018: loss improved from 2.36524 to 2.05830, saving model to model.h5\n",
      "Epoch 19/500\n",
      "7233/7233 [==============================] - 28s 4ms/step - loss: 1.7464 - acc: 0.5226\n",
      "\n",
      "Epoch 00019: loss improved from 2.05830 to 1.74643, saving model to model.h5\n",
      "Epoch 20/500\n",
      "7233/7233 [==============================] - 27s 4ms/step - loss: 1.4701 - acc: 0.5913\n",
      "\n",
      "Epoch 00020: loss improved from 1.74643 to 1.47006, saving model to model.h5\n",
      "Epoch 21/500\n",
      "7233/7233 [==============================] - 28s 4ms/step - loss: 1.2596 - acc: 0.6403\n",
      "\n",
      "Epoch 00021: loss improved from 1.47006 to 1.25956, saving model to model.h5\n",
      "Epoch 22/500\n",
      "7233/7233 [==============================] - 27s 4ms/step - loss: 0.9893 - acc: 0.7138\n",
      "\n",
      "Epoch 00022: loss improved from 1.25956 to 0.98926, saving model to model.h5\n",
      "Epoch 23/500\n",
      "7233/7233 [==============================] - 26s 4ms/step - loss: 0.8029 - acc: 0.7672\n",
      "\n",
      "Epoch 00023: loss improved from 0.98926 to 0.80286, saving model to model.h5\n",
      "Epoch 24/500\n",
      "7233/7233 [==============================] - 27s 4ms/step - loss: 0.6886 - acc: 0.7993\n",
      "\n",
      "Epoch 00024: loss improved from 0.80286 to 0.68863, saving model to model.h5\n",
      "Epoch 25/500\n",
      "7233/7233 [==============================] - 30s 4ms/step - loss: 0.5512 - acc: 0.8409\n",
      "\n",
      "Epoch 00025: loss improved from 0.68863 to 0.55124, saving model to model.h5\n",
      "Epoch 26/500\n",
      "7233/7233 [==============================] - 31s 4ms/step - loss: 0.3703 - acc: 0.8985\n",
      "\n",
      "Epoch 00026: loss improved from 0.55124 to 0.37032, saving model to model.h5\n",
      "Epoch 27/500\n",
      "7233/7233 [==============================] - 29s 4ms/step - loss: 0.2313 - acc: 0.9403\n",
      "\n",
      "Epoch 00027: loss improved from 0.37032 to 0.23132, saving model to model.h5\n",
      "Epoch 28/500\n",
      "7233/7233 [==============================] - 29s 4ms/step - loss: 0.2426 - acc: 0.9388\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.23132\n",
      "Epoch 00028: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d4aaa6320>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, callbacks=[es, ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T13:58:09.611166Z",
     "start_time": "2019-07-28T13:58:03.655904Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T13:58:09.625129Z",
     "start_time": "2019-07-28T13:58:09.613161Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    for i in range(num_gen_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        input_text += ' '+pred_word\n",
    "        output_text.append(pred_word)\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T14:00:17.703721Z",
     "start_time": "2019-07-28T14:00:17.580051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disobeying dinklage orders a young shae tyrion takes a joke for dorne and the character'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, seq_len, 'hand of the king', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
